ML Debugging Practice for Interview
This repository contains realistic ML debugging problems to help you prepare for technical interviews. Each problem has multiple intentional bugs that mirror real-world issues you might encounter.

ğŸ“ Files
ml_debugging_practice.py - Contains 8 buggy ML implementations
test_ml_debugging.py - Comprehensive unit tests that pass only when bugs are fixed
README.md - This file with setup instructions
ğŸš€ Quick Setup
1. Download Files
Save the three files to your local directory:

bash
# Create a new directory
mkdir ml_debugging_practice
cd ml_debugging_practice

# Copy the code files (from the artifacts above)
# ml_debugging_practice.py
# test_ml_debugging.py
# README.md
2. Install Dependencies
bash
pip install torch unittest2 psutil
3. Run Initial Tests (Should Fail)
bash
python test_ml_debugging.py
You should see many test failures - this is expected! Each failure points to a specific bug.

ğŸ¯ Practice Strategy
Phase 1: Bug Hunting (Day 1-2)
Read one problem at a time in ml_debugging_practice.py
Try to spot bugs by inspection before running tests
Run specific test classes to focus on one problem:
bash
python -m unittest test_ml_debugging.TestMultiHeadAttention -v
Phase 2: Systematic Debugging (Day 3-4)
Use test failures as hints - read the assertion messages carefully
Fix one bug at a time and re-run tests
Practice explaining your debugging process out loud
Phase 3: Interview Simulation (Day 5)
Have someone pick a random problem
Walk through your debugging methodology step-by-step
Explain what you're checking and why
ğŸ› Problems and Key Bugs
Problem 1: MultiHeadAttention
âŒ Missing attention scaling factor
âŒ Potential shape mismatches with different sequence lengths
Problem 2: PositionalEncoding
âŒ Registered as parameter instead of buffer
âŒ Gradient flow issues
Problem 3: Training Loop
âŒ Wrong gradient operation order
âŒ Memory leaks from storing tensor objects
âŒ Incorrect loss accumulation
Problem 4: KV Cache
âŒ Device mismatches
âŒ Cache overflow handling
âŒ Incorrect tensor indexing
Problem 5: Gradient Clipping
âŒ Wrong function name
âŒ Accumulating tensors instead of scalars
Problem 6: Transformer Decoder
âŒ Wrong normalization order (post-norm vs pre-norm)
âŒ Missing dropout in residual connections
Problem 7: Attention Masks
âŒ Incorrect causal mask creation
âŒ Broadcasting dimension errors
Problem 8: Model Evaluation
âŒ Missing model.eval() call
âŒ Missing torch.no_grad() context
âŒ Non-deterministic results
ğŸ§ª Running Tests
Run All Tests
bash
python test_ml_debugging.py
Run Specific Test Class
bash
python -m unittest test_ml_debugging.TestMultiHeadAttention -v
Run Single Test Method
bash
python -m unittest test_ml_debugging.TestMultiHeadAttention.test_attention_scaling -v
ğŸ’¡ Debugging Tips for Interview
1. Systematic Approach
Start with shapes: "Let me trace through the tensor dimensions..."
Check common issues: "I should verify the gradient flow..."
Use test failures as guides: "This test suggests the issue is..."
2. Think Out Loud
Explain your reasoning: "I'm checking this because..."
State your hypotheses: "I suspect the problem is..."
Describe your next steps: "Next, I would..."
3. Common ML Bug Categories
Tensor Operations: Shape mismatches, wrong transposes
Memory Management: Gradient accumulation, device mismatches
Training Logic: Wrong operation order, missing mode switches
Numerical Stability: Missing normalization, overflow issues
ğŸ¯ Interview Day Checklist
Before the Interview
 Can spot 2-3 bugs per problem in 5-10 minutes
 Can explain debugging methodology clearly
 Familiar with common PyTorch gotchas
 Practiced thinking out loud
During the Interview
 Start with high-level approach: "First, I'd check..."
 Be systematic: trace through the code step-by-step
 Ask clarifying questions: "What symptoms are you seeing?"
 Explain your reasoning for each check
ğŸ”§ Common Fix Patterns
Memory Leaks
python
# âŒ Wrong - stores tensor with gradients
losses.append(loss)

# âœ… Correct - stores scalar value
losses.append(loss.item())
Gradient Operations
python
# âŒ Wrong order
loss.backward()
optimizer.step()
optimizer.zero_grad()

# âœ… Correct order
optimizer.zero_grad()
loss.backward()
optimizer.step()
Model Modes
python
# âŒ Missing evaluation setup
def evaluate(model, data):
    for batch in data:
        output = model(batch)

# âœ… Proper evaluation setup
def evaluate(model, data):
    model.eval()
    with torch.no_grad():
        for batch in data:
            output = model(batch)
ğŸ“š Additional Resources
PyTorch Common Pitfalls
Transformer Implementation Guide
ML Debugging Best Practices
ğŸ‰ Success Criteria
You're ready for the interview when:

âœ… All unit tests pass
âœ… You can explain each bug and its fix
âœ… You can spot bugs quickly by inspection
âœ… You can articulate a systematic debugging approach
Good luck with your interview! ğŸš€

