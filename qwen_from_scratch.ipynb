{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f01b5dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12e6182",
   "metadata": {},
   "outputs": [],
   "source": [
    "QWEN3_CONFIG = {\n",
    "    \"d_model\": xx,\n",
    "    \"hidden_dim\": xx,\n",
    "    \"dtype\": xx,\n",
    "    \"qkv_bias\": False,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9647c2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    \"\"\"\n",
    "    SwiGLU(x, W1, W2, W3) = W3(SiLU(W1x) âŠ™ W2x) \n",
    "    \"\"\"\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(cfg[\"d_model\"], cfg[\"hidden_dim\"], dtype=cfg[\"dtype\"], bias=cfg[\"qkv_bias\"])\n",
    "        self.fc2 = nn.Linear(cfg[\"d_model\"], cfg[\"hidden_dim\"], dtype=cfg[\"dtype\"], bias=cfg[\"qkv_bias\"])\n",
    "        self.fc3 = nn.Linear(cfg[\"hidden_dim\"], cfg[\"d_model\"], dtype=cfg[\"dtype\"], bias=cfg[\"qkv_bias\"])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_fc1 = self.fc1(x)\n",
    "        x_fc2 = self.fc2(x)\n",
    "        x = nn.functional.silu(x_fc1) * x_fc2\n",
    "        return self.fc3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de031399",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, cfg, eps: float = 1e-6, bias: bool = False):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.scale = nn.Parameter(torch.ones(cfg[\"d_model\"]))\n",
    "        self.shift = nn.Parameter(torch.zeros(cfg[\"d_model\"])) if bias else None\n",
    "\n",
    "    def forward(self, x):\n",
    "        input_dtype = x.dtype\n",
    "        var = x.pow(2).mean(dim=-1, keepdim=True)\n",
    "        norm_x = x * torch.rsqrt(var + self.eps)\n",
    "        norm_x *= self.scale\n",
    "        if self.shift is not None:\n",
    "            norm_x += self.shift\n",
    "        return norm_x.to(input_dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc3e369",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RoPE(nn.Module):\n",
    "   def __init__(self, head_dim: int, theta_base: int = 10_000, context_length: int = 4096, dtype=torch.float32):\n",
    "        assert head_dim % 2 == 0, \"Embedding dimension must be even!\"\n",
    "\n",
    "        # theta = theta_base ** (-2i/d)\n",
    "        # where d = d_model; i = 0 -> d/2\n",
    "        token_pos = torch.arange(0, head_dim, 2, dtype=dtype)\n",
    "        # inv_freq = 1.0 / (theta_base ** token_pos[:(head_dim//2)].float() / head_dim)\n",
    "        inv_freq = 1.0 / theta_base ** (token_pos.float() / head_dim)\n",
    "        positions = torch.arange(context_length, dtype=dtype)         \n",
    "        angles = positions[:,None] * inv_freq[None,:] # (context_length, head_dim // 2)\n",
    "        angles = torch.cat([angles, angles], dim=1) # (context_length, head_dim)\n",
    "        \n",
    "        self.cos = torch.cos(angles)\n",
    "        self.sin = torch.sin(angles)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # only in MHSA, so the input shape is: \n",
    "        # (batch_size, n_heads, seq_len, head_dim)\n",
    "        batch_size, n_heads, seq_len, head_dim = x.shape\n",
    "\n",
    "        x1 = x[..., :head_dim//2] # first half\n",
    "        x2 = x[..., head_dim//2:] # second half\n",
    "\n",
    "        cos = self.cos[None,None,:seq_len,:]\n",
    "        sin = self.sin[None,None,:seq_len,:]\n",
    "\n",
    "        rotated = torch.cat((-x2, x1) ,dim=-1)\n",
    "        x_rotated = x * cos + rotated * sin\n",
    "        return x_rotated.to(dtype=x.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0bb03930",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 2, 4, 6, 8, 0, 2, 4, 6, 8]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat([\n",
    "    torch.arange(0,10,2)[None,:],\n",
    "    torch.arange(0,10,2)[None,:],\n",
    "], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f1c17d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e8c112",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_from_scratch (3.10.17)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
