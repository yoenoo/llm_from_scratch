{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ygcggh-VPClr"
      },
      "source": [
        "# **Background**\n",
        "\n",
        "Welcome to the C4AI Scholars Program Take-Home Challenge! This exercise is designed to allow you to showcase your engineering and problem solving skills. The Challenge consists of different challenges including:\n",
        "\n",
        "*   Identifying bugs, and getting the code working. This is designed to test your ability to grapple with real world engineering challenges.\n",
        "*   Testing your ability to generate code for a specified problem.\n",
        "*   An opportunity for you to attempt an optional challenge question that extends the original problem set.\n",
        "\n",
        "These tasks were chosen as a setting to see how you think about problems, even if they are not in your own research field of interest. The tasks and dataset are not meant to be indicative of the research goals of the Scholar Program. We purposefully have selected a simple toy problem so the focus is on how you think, and does not require significant machine learning resources (can be run in this colab).\n",
        "\n",
        "Good luck!\n",
        "\n",
        "**How to Use and Submit this Document?**\n",
        "\n",
        "*   **Make a copy of this document** and rename it **Firstname_Lastname_C4AIScholarsChallenge**\n",
        "*   Once you have completed all tasks, save and pin your revisions\n",
        "*   Submit the assignment by responding directly to this email with a link to your final document by Sunday, September 15th, 11 PM PDT."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_tjokNL8DVD2"
      },
      "source": [
        "## **Coding Challenge Part 1: Debugging custom SmolLM code [10 points]**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y6or_dbUDVD3"
      },
      "source": [
        "In this coding challenge, you are required to debug and fix a bare-bones implementation of the following model.\n",
        "\n",
        "**Model** : SmolLM-135M can be found at [HuggingFace](https://huggingface.co/HuggingFaceTB/SmolLM-135M).\n",
        "\n",
        "We have 10 bugs in the following implementation.\n",
        "There is a `check_solution` function for your convenience to verify you have correctly identified all the bugs. If you have found all bugs, the generated outputs will match the reference model exactly.\n",
        "\n",
        "**Rules**:\n",
        "1. **Bug Definition:**\n",
        "  - There are 10 bugs to be fixed.\n",
        "  - A bug is *defined as **{incorrect, missing, unnecessary}** lines of code*.\n",
        "  - You earn 1 point for each correctly identified and fixed bug.\n",
        "2. **Fix Guidelines:**\n",
        "  - You are encouraged to make the smallest possible fix, wherever possible (e.g. edit a line instead of replacing it entirely).\n",
        "  - Do not optimize the code; only fix the bugs. The implementation is *intentionally* non-optimized but valid.\n",
        "3. **Documentation:** Document each fix by adding a comment on the line above the fix: : `### BUG FIX ###`.\n",
        "4. **Sections:** *1. Setup [Helper Functions]* and *3. Test* don't contain bugs and shouldn't be changed.\n",
        "5. **Submission:** Your final submission should be the exact same file except with your proposed fixes and the respective comments as per Rule #3."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67d4055d-bb2d-49c7-a7e1-7f53fdb49658"
      },
      "source": [
        "## 1. Setup [Helper Functions]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "EIVmK-EX9KJa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Updated Git hooks.\n",
            "Git LFS initialized.\n",
            "Cloning into 'C4AI_SMOLLM135'...\n",
            "remote: Enumerating objects: 6, done.\u001b[K\n",
            "remote: Counting objects: 100% (3/3), done.\u001b[K\n",
            "remote: Compressing objects: 100% (3/3), done.\u001b[K\n",
            "remote: Total 6 (delta 0), reused 0 (delta 0), pack-reused 3 (from 1)\u001b[K\n",
            "Unpacking objects: 100% (6/6), 2.11 KiB | 721.00 KiB/s, done.\n",
            "2025_Scholar_Takehome_Exam.ipynb \u001b[1m\u001b[36mopenai_ml_debug_practice_v2\u001b[m\u001b[m\n",
            "BareBones_SmolLM-135M.pt         \u001b[1m\u001b[36mopenai_ml_debug_practice_v3\u001b[m\u001b[m\n",
            "\u001b[1m\u001b[36mC4AI_SMOLLM135\u001b[m\u001b[m                   \u001b[1m\u001b[36mscripts\u001b[m\u001b[m\n",
            "\u001b[1m\u001b[36mopenai_ml_debug_practice_v1\u001b[m\u001b[m\n"
          ]
        }
      ],
      "source": [
        "######################################################################################################################\n",
        "############################################## DO NOT CHANGE[START] ##################################################\n",
        "######################################################################################################################\n",
        "\n",
        "\n",
        "# [Don't use. Rate limit issues.] Use gdown to get weights file(BareBones_SmolLM-135M.pt) at https://drive.google.com/file/d/1tY46FSJEhGYRrfKRQTjJ1Cc7q9psaKUU/view . gdown should be installed by default else use `pip install gdown`\n",
        "# !gdown 1tY46FSJEhGYRrfKRQTjJ1Cc7q9psaKUU\n",
        "\n",
        "\n",
        "# [Recommended]Use HF to download the weights\n",
        "!git lfs install\n",
        "!git clone https://huggingface.co/dsouzadaniel/C4AI_SMOLLM135\n",
        "!mv C4AI_SMOLLM135/BareBones_SmolLM-135M.pt ./\n",
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "3d108504-1504-4ca7-a5ae-55975798b699"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "254fe45ff1c1407ba2b85939db64b462",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "02571318960e46e7895915949bc9583a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f8ca50d8b3f14ed29a3204f94cf17ec8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "merges.txt: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8e5bedf9494c4bddbfccf56e8b99b000",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f56f2124e38248518021464fdff6bcfc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/831 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "26f36dcfd8934e0c90f74a0d4bc009f7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/724 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3b7423332a774f61bff88c3991ca312f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/538M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3a109179e7224fceb16ba1470b11c356",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/111 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "\n",
        "# Libraries\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import nn\n",
        "import math\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# Model initialization/settings\n",
        "checkpoint=\"HuggingFaceTB/SmolLM-135M\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "\n",
        "__reference_model = AutoModelForCausalLM.from_pretrained(checkpoint)\n",
        "__reference_model.eval()\n",
        "\n",
        "class smolConfig:\n",
        "    vocab_size=49152\n",
        "    hidden_size=576\n",
        "    intermediate_size=1536\n",
        "    num_hidden_layers = 30\n",
        "    num_heads = 9\n",
        "    kv_heads=3\n",
        "config = smolConfig\n",
        "\n",
        "# Helper Functions\n",
        "def __generate(model, inputs, num_tokens):\n",
        "    collect = []\n",
        "    for _ in range(num_tokens):\n",
        "        output = model(**inputs)\n",
        "        output_id = torch.argmax(output['logits'][0,-1]).item()\n",
        "        collect.append(output_id)\n",
        "        if output_id==tokenizer.eos_token_id:\n",
        "            break\n",
        "        inputs['input_ids'] = torch.unsqueeze(torch.cat([inputs['input_ids'][0],torch.tensor([output_id])]),dim=0)\n",
        "        inputs['attention_mask'] = torch.ones_like(inputs['input_ids'])\n",
        "    return tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(collect))\n",
        "\n",
        "def check_solution(prompt, num_tokens, model_A, model_B):\n",
        "    print()\n",
        "    print(f\"{'>'*20}\\n\\tPrompt\\n{'<'*20}\\n{prompt}\\n\\n\")\n",
        "    model_inputs = tokenizer(prompt, return_tensors='pt')\n",
        "    print(f\"{'>'*30}\\n\\tModel_A Generation\\n{'<'*30}\\n{__generate(model_A,  model_inputs, num_tokens)}\")\n",
        "    print(\"\\n\\n\")\n",
        "    model_inputs = tokenizer(prompt, return_tensors='pt')\n",
        "    print(f\"{'>'*30}\\n\\tModel_B Generation\\n{'<'*30}\\n{__generate(model_B,  model_inputs, num_tokens)}\")\n",
        "\n",
        "######################################################################################################################\n",
        "############################################### DO NOT CHANGE[END] ###################################################\n",
        "######################################################################################################################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "62103e4d-e1b3-4687-a30a-65842ae1b8a9"
      },
      "source": [
        "## 2. Custom SmolLM (for BugFixes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ccfc40a-5fee-4f9a-81f3-8339a69ad5cf"
      },
      "outputs": [],
      "source": [
        "def rotate_half(x):\n",
        "    x1 = x[..., : x.shape[-1] // 2]\n",
        "    x2 = x[..., x.shape[-1] // 2 :]\n",
        "    return torch.cat((-x2, x1), dim=-1)\n",
        "\n",
        "def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n",
        "    cos = cos.unsqueeze(unsqueeze_dim)\n",
        "    sin = sin.unsqueeze(unsqueeze_dim)\n",
        "    q_embed = (q * cos) + (rotate_half(q) * sin)\n",
        "    k_embed = (k * cos) + (rotate_half(k) * sin)\n",
        "    return q_embed, k_embed\n",
        "\n",
        "def repeat_kv(hidden_states, n_rep):\n",
        "    batch, num_key_value_heads, slen, head_dim = hidden_states.shape\n",
        "    hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)\n",
        "    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)\n",
        "\n",
        "class RotaryEmbedder(nn.Module):\n",
        "    def __init__(self, dim, base):\n",
        "        super().__init__()\n",
        "        self.freq = 1/(base ** (torch.arange(0, dim, 2, dtype=torch.int64).float()/dim))\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def forward(self,x):\n",
        "        pos = torch.arange(x.shape[-2],dtype=torch.long)\n",
        "        # angles = torch.einsum('f,p->fp', self.freq, pos.float()).unsqueeze(dim=0)\n",
        "        angles = torch.einsum('f,p->fp', pos.float(), self.freq).unsqueeze(dim=0) ##\n",
        "\n",
        "        emb = torch.cat((angles, angles), dim=-1)\n",
        "        return emb.cos(), emb.sin()\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, hidden_size, intermediate_size):\n",
        "        super().__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.intermediate_size = intermediate_size\n",
        "        self.W_gate = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n",
        "        self.W_up = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n",
        "        self.W_down = nn.Linear(self.intermediate_size, self.hidden_size, bias=False)\n",
        "        self.act_fn = torch.nn.modules.activation.SiLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        down_proj = self.W_down(self.act_fn(self.W_gate(x)) * self.W_up(x)) ## \n",
        "        return down_proj\n",
        "\n",
        "class RMSNorm(nn.Module):\n",
        "    def __init__(self, hidden_size, eps=1e-6):\n",
        "        super().__init__()\n",
        "        self.weight = nn.Parameter(torch.ones(hidden_size))\n",
        "        self.variance_epsilon = eps\n",
        "\n",
        "    def forward(self, hidden_states):\n",
        "        variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
        "        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon) ##\n",
        "        return self.weight * hidden_states\n",
        "\n",
        "\n",
        "class RopeAttention(nn.Module):\n",
        "    def __init__(self,config):\n",
        "        super().__init__()\n",
        "        self.hidden_size=config.hidden_size\n",
        "        self.num_heads = config.num_heads\n",
        "        self.head_dim = config.hidden_size//self.num_heads\n",
        "        self.kv_heads = config.kv_heads\n",
        "        self.rope_theta = 10000.0\n",
        "\n",
        "        self.W_query = nn.Linear(config.hidden_size, self.num_heads * self.head_dim, bias=False)\n",
        "        self.W_key = nn.Linear(config.hidden_size, self.kv_heads * self.head_dim, bias=False)\n",
        "        self.W_value = nn.Linear(config.hidden_size, self.kv_heads * self.head_dim, bias=False)\n",
        "        self.W_output = nn.Linear(config.hidden_size, config.hidden_size, bias=False)\n",
        "        self.rotary_emb = RotaryEmbedder(base=self.rope_theta,\n",
        "                                         dim=self.head_dim) ## \n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        hidden_states: torch.Tensor,\n",
        "        attention_mask= None,\n",
        "    ):\n",
        "        b, q, _ = hidden_states.size()\n",
        "\n",
        "        q_states = self.W_query(hidden_states)\n",
        "        k_states = self.W_key(hidden_states)\n",
        "        v_states = self.W_value(hidden_states)\n",
        "\n",
        "        q_states = q_states.view(b, q, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        k_states = k_states.view(b, q, self.kv_heads, self.head_dim).transpose(1, 2)\n",
        "        v_states = v_states.view(b, q, self.kv_heads, self.head_dim).transpose(1, 2)\n",
        "\n",
        "        cos, sin = self.rotary_emb(v_states)\n",
        "        q_states, k_states = apply_rotary_pos_emb(q_states, k_states, cos, sin)\n",
        "\n",
        "        __kv_groups = self.num_heads // self.kv_heads ##\n",
        "        k_states = repeat_kv(k_states, __kv_groups)\n",
        "        v_states = repeat_kv(v_states, __kv_groups)\n",
        "\n",
        "        attn_weights = torch.matmul(q_states, k_states.transpose(2, 3)) / math.sqrt(self.head_dim) ## \n",
        "        attn_weights = attn_weights + attention_mask\n",
        "        # attn_weights.masked_fill_(attention_mask.bool(), -torch.inf)\n",
        "        attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n",
        "        attn_weights = nn.functional.dropout(attn_weights, training=self.training) ##\n",
        "\n",
        "        attn_output = torch.matmul(attn_weights, v_states)\n",
        "        attn_output = attn_output.transpose(1, 2).contiguous()\n",
        "        attn_output = attn_output.reshape(b, q, -1)\n",
        "\n",
        "        attn_output = self.W_output(attn_output) ##\n",
        "        return attn_output\n",
        "\n",
        "class LlamaDecoder(nn.Module):\n",
        "    def __init__(self,config):\n",
        "        super().__init__()\n",
        "        self.self_attn = RopeAttention(config)\n",
        "        self.mlp = MLP(hidden_size=config.hidden_size, intermediate_size=config.intermediate_size)\n",
        "        self.pre_attn_rmsnorm = RMSNorm(config.hidden_size, eps=1e-05)\n",
        "        self.pre_mlp_rmsnorm = RMSNorm(config.hidden_size, eps=1e-05)\n",
        "\n",
        "    def forward(self,hidden_states, attention_mask):\n",
        "        residual = hidden_states\n",
        "        hidden_states = self.pre_attn_rmsnorm(hidden_states)\n",
        "        attention_mask = torch.triu(torch.full((attention_mask.shape[-1],attention_mask.shape[-1]), fill_value=float('-inf')),diagonal=1)\n",
        "\n",
        "        hidden_states = self.self_attn(\n",
        "            hidden_states=hidden_states,\n",
        "            attention_mask=attention_mask,\n",
        "        )\n",
        "        hidden_states += residual\n",
        "        \n",
        "        residual = hidden_states ##\n",
        "        hidden_states = self.pre_mlp_rmsnorm(hidden_states)\n",
        "        hidden_states = self.mlp(hidden_states)\n",
        "        hidden_states += residual\n",
        "\n",
        "        outputs = (hidden_states,)\n",
        "\n",
        "        return outputs\n",
        "\n",
        "class smolModel(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.embed_tokens = nn.Embedding(num_embeddings=config.vocab_size,\n",
        "                                         embedding_dim=config.hidden_size)\n",
        "        self.layers = nn.ModuleList([LlamaDecoder(config) for _ in range(config.num_hidden_layers)])\n",
        "        self.norm = RMSNorm(config.hidden_size, eps=1e-05)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids= None,\n",
        "        attention_mask= None,\n",
        "    ):\n",
        "        inputs_embeds = self.embed_tokens(input_ids)\n",
        "        hidden_states = inputs_embeds\n",
        "        for decoder_layer in self.layers:\n",
        "            layer_outputs = decoder_layer(\n",
        "                hidden_states,\n",
        "                attention_mask=attention_mask,\n",
        "            )\n",
        "            hidden_states = layer_outputs[0]\n",
        "        hidden_states = self.norm(hidden_states)\n",
        "        return [hidden_states]\n",
        "\n",
        "class smolLM(nn.Module):\n",
        "    def __init__(self,config, reference_model_checkpoint=checkpoint):\n",
        "        super().__init__()\n",
        "        self.model = smolModel(config)\n",
        "        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n",
        "        if reference_model_checkpoint is not None:\n",
        "          # add lm_head from the reference model; currently only supports huggingface models\n",
        "          self.lm_head = AutoModelForCausalLM.from_pretrained(checkpoint).lm_head ##\n",
        "\n",
        "    def forward(self,input_ids,attention_mask):\n",
        "        outputs = self.model(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "        )\n",
        "        hidden_states = outputs[0] ## \n",
        "        logits = self.lm_head(hidden_states)\n",
        "        logits = logits.float()\n",
        "        return {'logits':logits}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "jPRkWRMLlGmD"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "smolLM(\n",
              "  (model): smolModel(\n",
              "    (embed_tokens): Embedding(49152, 576)\n",
              "    (layers): ModuleList(\n",
              "      (0-29): 30 x LlamaDecoder(\n",
              "        (self_attn): RopeAttention(\n",
              "          (W_query): Linear(in_features=576, out_features=576, bias=False)\n",
              "          (W_key): Linear(in_features=576, out_features=192, bias=False)\n",
              "          (W_value): Linear(in_features=576, out_features=192, bias=False)\n",
              "          (W_output): Linear(in_features=576, out_features=576, bias=False)\n",
              "          (rotary_emb): RotaryEmbedder()\n",
              "        )\n",
              "        (mlp): MLP(\n",
              "          (W_gate): Linear(in_features=576, out_features=1536, bias=False)\n",
              "          (W_up): Linear(in_features=576, out_features=1536, bias=False)\n",
              "          (W_down): Linear(in_features=1536, out_features=576, bias=False)\n",
              "          (act_fn): SiLU()\n",
              "        )\n",
              "        (pre_attn_rmsnorm): RMSNorm()\n",
              "        (pre_mlp_rmsnorm): RMSNorm()\n",
              "      )\n",
              "    )\n",
              "    (norm): RMSNorm()\n",
              "  )\n",
              "  (lm_head): Linear(in_features=576, out_features=49152, bias=False)\n",
              ")"
            ]
          },
          "execution_count": 61,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "__test_model = smolLM(config)\n",
        "__test_model.load_state_dict(torch.load('BareBones_SmolLM-135M.pt'), strict=False)\n",
        "__test_model.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d8d4fc28-1962-4f42-9378-22a683f4978d"
      },
      "source": [
        "# 3. Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "df559177-e725-41f6-8369-50e460165f78"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            ">>>>>>>>>>>>>>>>>>>>\n",
            "\tPrompt\n",
            "<<<<<<<<<<<<<<<<<<<<\n",
            "Given the following film movie by a critic, rate it out of 10. Respond in a single number.\n",
            "\n",
            "The movie started off extremely well, but just got worse after that.\n",
            "The storyline was all over the place and everyone acted terribly.\n",
            " 10/10 would not recommend! \n",
            "\n",
            " \n",
            "\n",
            "\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "\tModel_A Generation\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "1\n",
            "\n",
            "\n",
            "\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "\tModel_B Generation\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "1\n"
          ]
        }
      ],
      "source": [
        "######################################################################################################################\n",
        "############################################## DO NOT CHANGE[START] ##################################################\n",
        "######################################################################################################################\n",
        "\n",
        "###### TESTING PROMPTS\n",
        "# Single-Token Quick Test\n",
        "check_solution(prompt=\"Given the following film movie by a critic, rate it out of 10. Respond in a single number.\\n\\nThe movie started off extremely well, but just got worse after that.\\nThe storyline was all over the place and everyone acted terribly.\\n 10/10 would not recommend! \\n\\n \",\n",
        "               num_tokens=1,\n",
        "               model_A=__reference_model,\n",
        "               model_B=__test_model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "cQZd2U8naBGE"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            ">>>>>>>>>>>>>>>>>>>>\n",
            "\tPrompt\n",
            "<<<<<<<<<<<<<<<<<<<<\n",
            "Where is the Nile located?\n",
            "\n",
            "\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "\tModel_A Generation\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "\n",
            "The Nile River is located in the Nile Delta in the Nile River Basin, which is a region of Africa. It is the longest river in the world, with a length of 4,330 miles (6,900 km\n",
            "\n",
            "\n",
            "\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "\tModel_B Generation\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "\n",
            "The Nile River is located in the Nile Delta in the Nile River Basin, which is a region of Africa. It is the longest river in the world, with a length of 4,330 miles (6,900 km\n"
          ]
        }
      ],
      "source": [
        "# Multi-Token Quick Test\n",
        "check_solution(prompt=\"Where is the Nile located?\",\n",
        "               num_tokens=50,\n",
        "               model_A=__reference_model,\n",
        "               model_B=__test_model)\n",
        "\n",
        "######################################################################################################################\n",
        "############################################### DO NOT CHANGE[END] ###################################################\n",
        "######################################################################################################################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "28e4b530-1e73-4938-946a-5673b3a68244"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-uBeG0ZQQC8d"
      },
      "source": [
        "# **Coding Challenge Part 2: Teach SmolLM to do grammatical error correction [15 points]**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bSiBKZ9cU54t"
      },
      "source": [
        "The goal of this part is to train the SmolLM-135M model to perform grammatical error correction (GEC) using the Grammarly CoEdIT dataset. This [dataset](https://huggingface.co/datasets/grammarly/coedit), derived from the [CoEdIT project](https://arxiv.org/abs/2305.09857), provides a rich collection of text editing instructions and examples. The task involves several key steps that mimic conventional alignment processes:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CKCuZ-0Poi2u"
      },
      "source": [
        "## **2.1 Supervised Fine-Tuning (SFT) on Training Data [5 points]**\n",
        "\n",
        "* Fine-tune the [SmolLM-135M model](https://huggingface.co/HuggingFaceTB/SmolLM-135M) using the CoEdIT dataset, which includes input sentences with grammatical errors and their corrected versions. Use the training GEC portion of the CoEdIT dataset to teach the model how to correct grammatical errors effectively.\n",
        "* Calculate the BLEU score on the validation set to evaluate the model's performance in generating grammatically correct sentences. Ensure that this evaluation process is reusable for later comparisons.\n",
        "* Search for an optimal set of hyperparameters, such as the learning rate. We provide an estimated BLEU score that you should aim to achieve after one epoch. However, you may achieve a better score by finding the most suitable hyperparameters. **Do not train for more than 3 epochs -- we do not expect extensive training time.**\n",
        "* For Part 2, don't use additional libraries, if an imported library is missing, install it with **pip install**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7PAq9wgdQDpG"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Download the GEC data\n",
        "full_train_ds = load_dataset(\"grammarly/coedit\", split=\"train\")\n",
        "full_test_ds = load_dataset(\"grammarly/coedit\", split=\"validation\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2G5Zjn_7wKcN"
      },
      "outputs": [],
      "source": [
        "# TODO: Filter examples, keeping only GEC task\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RGDp47vndq9U"
      },
      "source": [
        "Expected number of train and test samples are 19823 and 485, respectively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D4KAJRaBhRnj"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "model_name = \"HuggingFaceTB/SmolLM-135M\"\n",
        "\n",
        "# TODO: Load the model and the tokenizer from huggingface\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RMqSVczarkSN"
      },
      "outputs": [],
      "source": [
        "# TRL - Transformer Reinforcement Learning -- https://huggingface.co/docs/trl/en/index\n",
        "from trl import SFTConfig, SFTTrainer\n",
        "\n",
        "# TODO: Run SFT\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RrwH3P8Q3hFc"
      },
      "outputs": [],
      "source": [
        "# Quick test if your model works properly\n",
        "def format_text(text: str) -> str:\n",
        "    # here you may have formatting of the input that you adopted for training\n",
        "    return text\n",
        "\n",
        "\n",
        "# Example of how to run inference on a single example\n",
        "text = \"Fix grammatically: I likes turtles\"\n",
        "inputs = tokenizer(format_text(text), return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n",
        "outputs = model.generate(**inputs, max_new_tokens=128, temperature=0.0)\n",
        "print(tokenizer.decode(outputs[0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Nat68x64Wlh"
      },
      "source": [
        "Expected output: I like turtles."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QTIrCDJOpUFw"
      },
      "outputs": [],
      "source": [
        "import evaluate\n",
        "\n",
        "# BLEU Score\n",
        "def evaluate_model(model, tokenizer, ds):\n",
        "    # TODO - compute and call preds and targets for the bleu.compute in the following.\n",
        "\n",
        "\n",
        "    bleu = evaluate.load(\"bleu\")\n",
        "    results = bleu.compute(predictions=preds, references=targets)\n",
        "    return results[\"bleu\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m2d-nJpLqcaF"
      },
      "outputs": [],
      "source": [
        "# TODO: Evaluate model, use the function given above\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UGZ12G_TbweV"
      },
      "source": [
        "Expected BLEU score after 1 epoch SFT is ~ 0.48."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MNL0C_cnoV0E"
      },
      "source": [
        "## **2.2 Create a preference optimization dataset [5 points]**\n",
        "\n",
        "* *Generate Output Variants* -- for each input sentence in the training set, use the fine-tuned model to generate two different output variants.\n",
        " * Consider using different decoding strategies, such as varying the temperature or beam size, to produce diverse outputs. Select an approach based on the desired balance between diversity and quality.\n",
        "\n",
        "* *Preference Annotation* -- measure the edit distance between each **generated predicted variant** and **ground truth correction**. Label the variant with the lower edit distance as \"chosen\" and the one with the higher edit distance as \"rejected.\"\n",
        " * Beyond using edit distance, what other metrics or methods could you consider to do preference dataset annotation?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "krzvaoN-o5mf"
      },
      "outputs": [],
      "source": [
        "from fast_edit_distance import edit_distance\n",
        "\n",
        "# TODO: Create preference optimization dataset\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mcPiDof4iLNP"
      },
      "outputs": [],
      "source": [
        "# TODO: (Load and) Visualize the created dataset -- display at least 5 lines of the dataset.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s2uLgjo7oaaH"
      },
      "source": [
        "## **2.3 Run Direct Preference Optimization (DPO) [5 points]**\n",
        "* Use the preference optimization dataset to further train the model through DPO, a method that leverages human-like preferences for model training.\n",
        "* After running DPO, measure the BLEU score on the test set. Compare this performance to the baseline established during the SFT phase.\n",
        "* Search for an optimal set of hyperparameters, such as the learning rate and number of epochs. We provide an estimated BLEU score that you should aim to achieve after one epoch. However, you may achieve a better score by finding the most suitable hyperparameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "36t5EBK8o_sE"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from trl import DPOConfig, DPOTrainer\n",
        "from transformers import AutoModelForCausalLM\n",
        "from datasets import Dataset\n",
        "import pandas as pd\n",
        "\n",
        "# TODO: Run Direct Preference Optimization (DPO)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ST7kFAOhmOH"
      },
      "outputs": [],
      "source": [
        "# TODO: Evaluate model, use evaluate_model function\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pffG5Flhb5Nu"
      },
      "source": [
        "Expected BLEU score after 1 epoch SFT + DPO is ~ 0.50."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1LzPSeY24tM-"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UeBKJjRjsfHa"
      },
      "source": [
        "# **Coding Challenge Part 3: Explore Alternative DPO Variants for Improved Model Performance [10 points]**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z6KoS4jB01WW"
      },
      "source": [
        "Consider employing a different version or variant of DPO. Your task is to:\n",
        "\n",
        "* Choose a variant of DPO or another preference-based optimization method that could potentially enhance the model's performance.\n",
        "* Describe the specific differences in this approach compared to the initial DPO method used.\n",
        "* Train the model using this alternative DPO method and measure its performance on the test set using the BLEU score.\n",
        "* Compare these results with the baseline performance achieved during the initial Supervised Fine-Tuning (SFT) and the first DPO implementation.\n",
        "* Select a few GEC example after SFT, DPO and this DPO variant phases and compare the quality of the corrections, which one you prefer as human?\n",
        "* You are allowed to make changes in the preference data annotation to improve the score, e.g. apply different metrics or methods beyond edit distance.\n",
        "* Discuss the role of any changes in achieving these results. Consider potential trade-offs or limitations introduced by the new approach."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FFeIb7w6qrDw"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gkBlDq0Bi8HW"
      },
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "llm_from_scratch (3.10.17)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.17"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
